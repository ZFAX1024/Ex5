{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4422d823-f3ba-4b67-9be3-91089a274e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size = 512\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = transforms.Compose ([transforms.ToTensor() ])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader (\n",
    "    datasets.MNIST('data', train=True, download=True, transform=transform), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "testloader = torch.utils.data.DataLoader (\n",
    "    datasets.MNIST('data', train=False, download=True, transform=transform), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b052a428-6e04-4622-9c4f-bf77e9a5a826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (clf): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # self.conv1=#输入通道1,输出通道6,卷积核5x5\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5) \n",
    "        # self.conv2=#输入通道6,输出通道16,卷积核5x5\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # self.fc1 = #全连接层,输入5*5*16,输出120\n",
    "        self.fc1 = nn.Linear(5 * 5 * 16, 120)\n",
    "        # self.fc2 = #全连接层,输入120,输出84\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        # self.clf = #分类层,输入84,输出10\n",
    "        self.clf = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.pad(x, (2, 2, 2, 2), \"constant\", 0) \n",
    "        # conv1\n",
    "        x = self.conv1(x)\n",
    "        # 激活函数sigmoid()\n",
    "        x = F.sigmoid(x)\n",
    "        # 平均池化层,kernel = 2x2,步长2\n",
    "        x = F.avg_pool2d(x, kernel_size=2, stride=2)\n",
    "        \n",
    "        # conv2\n",
    "        x = self.conv2(x)\n",
    "        # 激活函数sigmoid()\n",
    "        x = F.sigmoid(x)\n",
    "        # 平均池化层,2x2,步长2\n",
    "        x = F.avg_pool2d(x, kernel_size=2, stride=2)\n",
    "        \n",
    "        # 展平,从第1维开始展平\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # 全连接层1\n",
    "        x = self.fc1(x)\n",
    "        # 激活函数sigmoid()\n",
    "        x = F.sigmoid(x)\n",
    "        \n",
    "        # 全连接层2\n",
    "        x = self.fc2(x)\n",
    "        # 激活函数sigmoid()\n",
    "        x = F.sigmoid(x)\n",
    "        \n",
    "        # 分类层\n",
    "        x = self.clf(x)\n",
    "        return x\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f311c509-0daf-4d8e-a138-764da396073a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 2.3012, acc:0.1135\n",
      "epoch: 1, loss: 2.3014, acc:0.1135\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 10\n",
    "accs, losses = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (x, y) in enumerate(trainloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    testloss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, y) in enumerate(testloader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            testloss += F.cross_entropy(out, y).item()\n",
    "            pred = out.max(dim=1, keepdim=True)[1]\n",
    "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "            \n",
    "    acc = correct / len(testloader.dataset)\n",
    "    testloss = testloss / (batch_idx + 1)\n",
    "    accs.append(acc)\n",
    "    losses.append(testloss)\n",
    "    \n",
    "    print('epoch: {}, loss: {:.4f}, acc:{:.4f}'.format(epoch, testloss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b84683-90b9-4df0-885c-f38aeff1f44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 4: 特征图可视化 (超保守版，严格遵循原始文件代码行 91-97)\n",
    "\n",
    "# %%\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # 1. 临时加载一个 batch\n",
    "    x_temp, _ = next(iter(testloader)) \n",
    "    x = x_temp.to(device)\n",
    "\n",
    "    # 2. 严格执行 LeNet-5 的填充\n",
    "    x = F.pad(x, (2, 2, 2, 2), \"constant\", 0) \n",
    "    \n",
    "    # 3. 严格执行原始文件中的行 131-137 的特征提取，但这次我们不立即池化\n",
    "    feature1 = F.sigmoid(model.conv1(x))\n",
    "    # feature1 = F.avg_pool2d(feature1, kernel_size=2, stride=2) # 原始代码中注释掉了\n",
    "    feature2 = F.sigmoid(model.conv2(feature1))\n",
    "    # feature2 = F.avg_pool2d(feature2, kernel_size=2, stride=2) # 原始代码中注释掉了\n",
    "\n",
    "    # 设置 N=5，严格对应原始文件行 91\n",
    "    n = 5\n",
    "\n",
    "    # 4. 仅将所需的前 N 个样本转移到 CPU (原始文件行 92-94)\n",
    "    # x 此时是 512 个填充后的样本，我们取前 n 个原始样本进行可视化\n",
    "    # 注意: 为了显示原始图像，我们应该使用未填充的 x_temp\n",
    "    img = x_temp.detach().cpu().numpy()[:n]\n",
    "    feature_map1 = feature1.detach().cpu().numpy()[:n]\n",
    "    feature_map2 = feature2.detach().cpu().numpy()[:n]\n",
    "\n",
    "# 5. 绘图 (严格对应原始文件行 96-97)\n",
    "fig, ax = plt.subplots(3, n, figsize=(10, 10))\n",
    "\n",
    "for i in range(n):\n",
    "    # 原始图像 (ax[0,i] 之前可能是 ax[0,1])\n",
    "    # 注意：为了让图片按 i 顺序排列，这里使用 ax[0, i] 而不是 ax[0, 1]\n",
    "    ax[0, i].imshow(img[i].sum(0), cmap='gray') \n",
    "    ax[0, i].axis('off')\n",
    "    \n",
    "    # 第一层特征图\n",
    "    ax[1, i].imshow(feature_map1[i].sum(0), cmap='gray')\n",
    "    ax[1, i].axis('off')\n",
    "    \n",
    "    # 第二层特征图\n",
    "    ax[2, i].imshow(feature_map2[i].sum(0), cmap='gray')\n",
    "    ax[2, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5860de8e-647d-46f6-a67b-548e94a6fda8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285621b-0665-4753-8098-9059622f371b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
